{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install torch==2.1.0 torchvision==0.16.0 --upgrade --quiet\n",
    "!pip install pytorchvideo accelerate --upgrade --quiet"
   ],
   "metadata": {
    "id": "Q7253JVr86As",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a2c22006-ecb7-4d18-f9c8-e3de24cdd498"
   },
   "id": "Q7253JVr86As",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ez6t28we7mdq",
    "outputId": "b3fda42d-9da3-4e01-e7e7-cc7250003d62"
   },
   "id": "ez6t28we7mdq",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install \\\n",
    "    python-dotenv \\\n",
    "    opencv-python \\\n",
    "    timesformer-pytorch \\\n",
    "    transformers==4.48.0 \\\n",
    "    accelerate \\\n",
    "    pytorchvideo \\\n",
    "    imageio \\\n",
    "    ipython \\\n",
    "    evaluate \\\n",
    "    decord \\\n",
    "    av"
   ],
   "metadata": {
    "collapsed": true,
    "id": "ukhkOvB57V5q",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4a6ca644-3432-4ee0-b937-875afa22f332"
   },
   "id": "ukhkOvB57V5q",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install numpy==1.26.1 --upgrade --quiet"
   ],
   "metadata": {
    "id": "rBUW7d9H_5hq",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d025e8bd-f268-4c97-bea2-b5eccc4fdb12"
   },
   "id": "rBUW7d9H_5hq",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "initial_id",
    "outputId": "11421d62-83c3-41b8-f4bf-c2b8af7de5be"
   },
   "source": [
    "import os\n",
    "import av\n",
    "\n",
    "import torch\n",
    "from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\n",
    "import pytorchvideo.data\n",
    "\n",
    "import imageio\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    Resize,\n",
    ")\n",
    "\n",
    "import evaluate\n",
    "from transformers import TrainingArguments, Trainer"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import av\n",
    "\n",
    "import torch\n",
    "from pytorchvideo.transforms import UniformTemporalSubsample\n",
    "from torch.utils.data import Dataset\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "from typing import Callable, Dict\n",
    "\n",
    "class DeceptionDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        video_label_map: Dict[str, int],\n",
    "        transform: Callable = None,\n",
    "        num_frames: int = 16,\n",
    "    ):\n",
    "        self.video_label_map = video_label_map\n",
    "        self.video_paths = list(video_label_map.keys())\n",
    "        self.transform = transform\n",
    "        self.num_frames = num_frames\n",
    "        self.num_videos = len(video_label_map)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        video_path = self.video_paths[index]\n",
    "        label = self.video_label_map[video_path]\n",
    "\n",
    "        video = EncodedVideo.from_path(video_path, decoder=\"decord\")\n",
    "        video_data = video.get_clip(0, video.duration)\n",
    "        video_tensor = video_data['video']  # shape: (C, T, H, W)\n",
    "\n",
    "        if video_tensor is None:\n",
    "            raise ValueError(f\"Could not load video: {video_path}\")\n",
    "\n",
    "        C, T, H, W = video_tensor.shape\n",
    "        if T < self.num_frames:\n",
    "            pad = video_tensor[:, -1:].repeat(1, self.num_frames - T, 1, 1)\n",
    "            video_tensor = torch.cat([video_tensor, pad], dim=1)\n",
    "\n",
    "        # Subsample T frames uniformly\n",
    "        video_clip = UniformTemporalSubsample(self.num_frames)(video_tensor)  # still (C, T, H, W)\n",
    "\n",
    "        if self.transform:\n",
    "            video_clip = self.transform({\"video\": video_clip})[\"video\"]\n",
    "\n",
    "        return {\n",
    "            \"video\": video_clip,  # torch.Tensor of shape (C, T, H, W)\n",
    "            \"label\": label        # 0 (truth) or 1 (lie)\n",
    "        }\n"
   ],
   "metadata": {
    "id": "t4ZNDkSj8VX2"
   },
   "id": "t4ZNDkSj8VX2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "def build_clip_label_map(video_clips_dir: str) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Maps each video clip path to its deception label (0 = truth, 1 = lie)\n",
    "    based solely on its filename (e.g., trial_lie_002_002.mp4).\n",
    "\n",
    "    Args:\n",
    "        video_clips_dir (str): Path to directory containing segmented video clips.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, int]: Dictionary mapping each clip path to its numeric label.\n",
    "    \"\"\"\n",
    "    clip_to_label = {}\n",
    "    for fname in sorted(os.listdir(video_clips_dir)):  # <- added sorted() here\n",
    "        if not fname.endswith(\".mp4\"):\n",
    "            continue\n",
    "\n",
    "        parts = fname.split('_')\n",
    "        if len(parts) < 3:\n",
    "            print(f\"Skipping malformed filename: {fname}\")\n",
    "            continue\n",
    "\n",
    "        label_str = parts[1]\n",
    "        label = 1 if label_str.lower() == \"lie\" else 0\n",
    "        path = os.path.join(video_clips_dir, fname)\n",
    "        clip_to_label[path] = label\n",
    "\n",
    "    return clip_to_label\n"
   ],
   "metadata": {
    "id": "GXPyq4BQ8bDr"
   },
   "id": "GXPyq4BQ8bDr",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a83c435178854ce0",
   "metadata": {
    "id": "a83c435178854ce0"
   },
   "source": [
    "ROOT_DIR = \"/content/drive/MyDrive/deception_detection\"\n",
    "video_clips_dir_train = os.path.join(ROOT_DIR, \"train\")\n",
    "video_clips_dir_val = os.path.join(ROOT_DIR, \"val\")\n",
    "video_clips_dir_test = os.path.join(ROOT_DIR, \"test\")\n",
    "\n",
    "clip_to_label_train = build_clip_label_map(video_clips_dir_train)\n",
    "clip_to_label_val = build_clip_label_map(video_clips_dir_val)\n",
    "clip_to_label_test = build_clip_label_map(video_clips_dir_test)\n",
    "\n",
    "class_labels = {\"truth\", \"lie\"}\n",
    "label2id = {label: i for i, label in enumerate(class_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2988a538b7d7fd3f",
   "metadata": {
    "id": "2988a538b7d7fd3f",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1b6a0e24-af69-4344-b58c-170af929b468"
   },
   "source": [
    "model_ckpt = \"MCG-NJU/videomae-large\" #OpenGVLab/VideoMAEv2-Huge\" #\"MCG-NJU/videomae-base\"\n",
    "image_processor = VideoMAEImageProcessor.from_pretrained(model_ckpt)\n",
    "model = VideoMAEForVideoClassification.from_pretrained(\n",
    "    model_ckpt,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fb6b79be4af21674",
   "metadata": {
    "id": "fb6b79be4af21674",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7cf96981-4d7c-4404-89cb-781f17ee2b7c"
   },
   "source": [
    "mean = image_processor.image_mean\n",
    "std = image_processor.image_std\n",
    "if \"shortest_edge\" in image_processor.size:\n",
    "    height = width = image_processor.size[\"shortest_edge\"]\n",
    "else:\n",
    "    height = image_processor.size[\"height\"]\n",
    "    width = image_processor.size[\"width\"]\n",
    "resize_to = (height, width)\n",
    "\n",
    "num_frames_to_sample = model.config.num_frames\n",
    "#num_frames_to_sample = 16\n",
    "print(num_frames_to_sample)\n",
    "sample_rate = 4\n",
    "fps = 30\n",
    "clip_duration = num_frames_to_sample * sample_rate / fps"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "95f6a3d0e2287144",
   "metadata": {
    "id": "95f6a3d0e2287144"
   },
   "source": [
    "train_transform = Compose(\n",
    "    [\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    UniformTemporalSubsample(num_frames_to_sample),\n",
    "                    Lambda(lambda x: x / 255.0),\n",
    "                    Normalize(mean, std),\n",
    "                    RandomShortSideScale(min_size=256, max_size=320),\n",
    "                    RandomCrop(resize_to),\n",
    "                    RandomHorizontalFlip(p=0.5),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "train_dataset = DeceptionDataset(\n",
    "    video_label_map=clip_to_label_train,\n",
    "    transform=train_transform,\n",
    "    num_frames=num_frames_to_sample\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f8be97fd791ef60e",
   "metadata": {
    "id": "f8be97fd791ef60e"
   },
   "source": [
    "val_transform = Compose(\n",
    "    [\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    UniformTemporalSubsample(num_frames_to_sample),\n",
    "                    Lambda(lambda x: x / 255.0),\n",
    "                    Normalize(mean, std),\n",
    "                    Resize(resize_to),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_dataset = DeceptionDataset(\n",
    "    video_label_map=clip_to_label_val,\n",
    "    transform=val_transform\n",
    ")\n",
    "\n",
    "test_dataset = DeceptionDataset(\n",
    "    video_label_map=clip_to_label_test,\n",
    "    transform=val_transform\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3bd0bb3d19a2d4f6",
   "metadata": {
    "id": "3bd0bb3d19a2d4f6"
   },
   "source": [
    "def unnormalize_img(img):\n",
    "    \"\"\"Un-normalizes the image pixels.\"\"\"\n",
    "    img = (img * std) + mean\n",
    "    img = (img * 255).astype(\"uint8\")\n",
    "    return img.clip(0, 255)\n",
    "\n",
    "\n",
    "def create_gif(video_tensor, filename=\"sample.gif\"):\n",
    "    \"\"\"Prepares a GIF from a video tensor.\n",
    "    The video tensor is expected to have the following shape:\n",
    "    (num_frames, num_channels, height, width).\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    for video_frame in video_tensor:\n",
    "        frame_unnormalized = unnormalize_img(video_frame.permute(1, 2, 0).numpy())\n",
    "        frames.append(frame_unnormalized)\n",
    "    kargs = {\"duration\": 0.25}\n",
    "    imageio.mimsave(filename, frames, \"GIF\", **kargs)\n",
    "    return filename\n",
    "\n",
    "\n",
    "def display_gif(video_tensor, gif_name=\"sample.gif\"):\n",
    "    \"\"\"Prepares and displays a GIF from a video tensor.\"\"\"\n",
    "    video_tensor = video_tensor.permute(1, 0, 2, 3)\n",
    "    gif_filename = create_gif(video_tensor, gif_name)\n",
    "    return Image(filename=gif_filename)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "16c3a4b1ea645579",
   "metadata": {
    "id": "16c3a4b1ea645579"
   },
   "source": [
    "model_name = model_ckpt.split(\"/\")[-1]\n",
    "new_model_name = f\"{model_name}-finetuned-deception-dataset_v2\"\n",
    "num_epochs = 1\n",
    "batch_size = 5\n",
    "gradient_accumulation_steps = 8\n",
    "\n",
    "args = TrainingArguments(\n",
    "    new_model_name,\n",
    "    remove_unused_columns=False,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=True,\n",
    "    max_steps=(train_dataset.num_videos // batch_size) * num_epochs,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "15c430d82969903e",
   "metadata": {
    "id": "15c430d82969903e"
   },
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "\n",
    "\n",
    "def collate_fn(examples):\n",
    "    # permute to (num_frames, num_channels, height, width)\n",
    "    pixel_values = torch.stack(\n",
    "        [example[\"video\"].permute(1, 0, 2, 3) for example in examples]\n",
    "    )\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "951054ac5a0c4301a7f56fc6e5566fb3",
      "981c0d881ff74e5eb1558f07122569f0",
      "52ee1b036150436ea59dd5c6cb9726b9",
      "b76377cb8cae4b9da8e4db54fc11925a",
      "dfd3b11118ad4ae2ade6566d3e05de78",
      "2063d03e7309451497264575765dfdbd",
      "60229e31ccbc4d2086ac2302f3a0b985",
      "0aedd675651a4df7b3965af2a03cc93b",
      "230154151a4f4eb88a926c39c117fe94",
      "736fbc68ab7746fc80f932d41b6e6903",
      "0d880a9252974d76934efaba718e5608",
      "9e2a0927bbd14af29ba2f9860fbfb6a7",
      "bcdcba3f45c347a08e5a3de43f481919",
      "7b97da292510480caf55c8bf770c10bc",
      "dd086627509a413595f9c8b969458bc8",
      "987d738767b74d5a977caa67281a9778",
      "1b770f6a2d0f43efa68e84db257d34a6",
      "88094895eba644cd8784c25ea5f797cc",
      "75946fcf419b4c8abccf51e4f65a7a0f",
      "01e62f01ce9941b489c1588e67698698"
     ]
    },
    "id": "Lhpgk-HSrfJi",
    "outputId": "04b971f9-2c0e-4a44-c4eb-67a1aecdb3c9"
   },
   "id": "Lhpgk-HSrfJi",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cf8c9343e71c44cd",
   "metadata": {
    "id": "cf8c9343e71c44cd"
   },
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=image_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a299eaf0385432f8",
   "metadata": {
    "id": "a299eaf0385432f8",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "outputId": "a3c532f7-71c8-4214-af6a-ca5a47ebcb33"
   },
   "source": [
    "train_results = trainer.train()\n",
    "trainer.save_model(\"/content/drive/MyDrive/my_finetuned_model-mae-large-v2\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_direct_model(model, dataloader, device: str = \"cuda\"):\n",
    "    \"\"\"\n",
    "    Evaluate a Hugging Face video model directly on a tensor-based dataloader.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Pretrained Hugging Face model loaded via AutoModelForVideoClassification.\n",
    "        dataloader (DataLoader): PyTorch dataloader yielding {\"video\": tensor, \"label\": int}.\n",
    "        device (str): \"cuda\" or \"cpu\".\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy over the dataset.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for sample in tqdm(dataloader):\n",
    "        video = sample[\"video\"].squeeze(0)  # (C, T, H, W)\n",
    "        label = sample[\"label\"]\n",
    "\n",
    "        # Prepare input\n",
    "        video = video.permute(1, 0, 2, 3)  # (T, C, H, W)\n",
    "        inputs = {\n",
    "            \"pixel_values\": video.unsqueeze(0).to(device)  # add batch dim\n",
    "        }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            pred_label_id = logits.argmax(dim=-1).item()\n",
    "\n",
    "        if pred_label_id == label:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Direct model accuracy: {accuracy:.4f}\")\n",
    "    return accuracy\n"
   ],
   "metadata": {
    "id": "vHqmO1o6Ql3f"
   },
   "id": "vHqmO1o6Ql3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_scene_id_from_path(video_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts a scene/group ID from the video filename.\n",
    "    Assumes filenames follow pattern: trial_lie_002_003.mp4 → scene ID: trial_lie_002\n",
    "    \"\"\"\n",
    "    filename = Path(video_path).stem  # e.g., trial_lie_002_003\n",
    "    parts = filename.split('_')\n",
    "    return '_'.join(parts[:3])  # trial + lie + 002 = trial_lie_002\n",
    "\n",
    "def evaluate_scene_level_model(model, dataloader, video_paths, device: str = \"cuda\"):\n",
    "    \"\"\"\n",
    "    Evaluates model using majority voting at the scene level.\n",
    "\n",
    "    Args:\n",
    "        model: Hugging Face video classification model.\n",
    "        dataloader: DataLoader that yields {\"video\": tensor, \"label\": int}.\n",
    "        video_paths: Ordered list of video file paths from the dataset.\n",
    "        device: \"cuda\" or \"cpu\"\n",
    "\n",
    "    Returns:\n",
    "        float: scene-level accuracy\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    scene_preds = defaultdict(list)\n",
    "    scene_labels = {}\n",
    "\n",
    "    for idx, sample in enumerate(tqdm(dataloader)):\n",
    "        video = sample[\"video\"].squeeze(0)  # (C, T, H, W)\n",
    "        label = sample[\"label\"]\n",
    "        video_path = video_paths[idx]\n",
    "        scene_id = extract_scene_id_from_path(video_path)\n",
    "\n",
    "        video = video.permute(1, 0, 2, 3)  # (T, C, H, W)\n",
    "        inputs = {\"pixel_values\": video.unsqueeze(0).to(device)}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # logits = model(**inputs).logits\n",
    "            outputs = model(**inputs, output_attentions=True)\n",
    "            logits = outputs.logits.squeeze(0).cpu()\n",
    "            attentions = outputs.attentions  # List of attention tensors\n",
    "            pred_label = logits.argmax(dim=-1).item()\n",
    "\n",
    "        scene_preds[scene_id].append(pred_label)\n",
    "        scene_labels[scene_id] = label  # Set once per scene\n",
    "\n",
    "    correct = 0\n",
    "    for scene_id, preds in scene_preds.items():\n",
    "        majority = Counter(preds).most_common(1)[0][0]\n",
    "        true = scene_labels[scene_id]\n",
    "        if majority == true:\n",
    "            correct += 1\n",
    "\n",
    "    total = len(scene_preds)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Scene-level accuracy (majority vote): {accuracy:.4f}\")\n",
    "    return accuracy\n"
   ],
   "metadata": {
    "id": "IUQ_NYdHcpqf"
   },
   "id": "IUQ_NYdHcpqf",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_scene_id_from_path(video_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts a scene/group ID from the video filename.\n",
    "    Assumes filenames follow pattern: trial_lie_002_003.mp4 → scene ID: trial_lie_002\n",
    "    \"\"\"\n",
    "    filename = Path(video_path).stem\n",
    "    parts = filename.split('_')\n",
    "    return '_'.join(parts[:3])\n",
    "\n",
    "def evaluate_scene_level_model_logit_sum(model, dataloader, video_paths, device: str = \"cuda\"):\n",
    "    \"\"\"\n",
    "    Evaluates model using confidence (logit) sum voting at the scene level.\n",
    "\n",
    "    Args:\n",
    "        model: Hugging Face video classification model.\n",
    "        dataloader: DataLoader yielding {\"video\": tensor, \"label\": int}.\n",
    "        video_paths: Ordered list of video file paths from the dataset.\n",
    "        device: \"cuda\" or \"cpu\"\n",
    "\n",
    "    Returns:\n",
    "        float: Scene-level accuracy\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    scene_logits = defaultdict(list)\n",
    "    scene_labels = {}\n",
    "\n",
    "    for idx, sample in enumerate(tqdm(dataloader)):\n",
    "        video = sample[\"video\"].squeeze(0)  # (C, T, H, W)\n",
    "        label = sample[\"label\"]\n",
    "        video_path = video_paths[idx]\n",
    "        scene_id = extract_scene_id_from_path(video_path)\n",
    "\n",
    "        video = video.permute(1, 0, 2, 3)  # (T, C, H, W)\n",
    "        inputs = {\"pixel_values\": video.unsqueeze(0).to(device)}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits.squeeze(0).cpu()  # (num_classes,)\n",
    "\n",
    "        scene_logits[scene_id].append(logits)\n",
    "        scene_labels[scene_id] = label\n",
    "\n",
    "    correct = 0\n",
    "    for scene_id, logits_list in scene_logits.items():\n",
    "        total_logits = torch.stack(logits_list).sum(dim=0)  # (num_classes,)\n",
    "        final_prediction = total_logits.argmax().item()\n",
    "        true_label = scene_labels[scene_id]\n",
    "\n",
    "        if final_prediction == true_label:\n",
    "            correct += 1\n",
    "        else:\n",
    "          print(scene_id)\n",
    "\n",
    "    total = len(scene_logits)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Scene-level accuracy (logit-sum voting): {accuracy:.4f}\")\n",
    "    return accuracy\n"
   ],
   "metadata": {
    "id": "qII15wI3jVr7"
   },
   "id": "qII15wI3jVr7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_scene_id_from_path(video_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts a scene/group ID from the video filename.\n",
    "    Assumes filenames follow pattern: trial_lie_002_003.mp4 → scene ID: trial_lie_002\n",
    "    \"\"\"\n",
    "    filename = Path(video_path).stem\n",
    "    parts = filename.split('_')\n",
    "    return '_'.join(parts[:3])\n",
    "\n",
    "def evaluate_scene_level_model_softmax_sum(model, dataloader, video_paths, device: str = \"cuda\"):\n",
    "    \"\"\"\n",
    "    Evaluates model using probability (softmax) sum voting at the scene level.\n",
    "\n",
    "    Args:\n",
    "        model: Hugging Face video classification model.\n",
    "        dataloader: DataLoader yielding {\"video\": tensor, \"label\": int}.\n",
    "        video_paths: Ordered list of video file paths from the dataset.\n",
    "        device: \"cuda\" or \"cpu\"\n",
    "\n",
    "    Returns:\n",
    "        float: Scene-level accuracy\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    scene_logits = defaultdict(list)\n",
    "    scene_labels = {}\n",
    "\n",
    "    for idx, sample in enumerate(tqdm(dataloader)):\n",
    "        video = sample[\"video\"].squeeze(0)  # (C, T, H, W)\n",
    "        label = sample[\"label\"]\n",
    "        video_path = video_paths[idx]\n",
    "        scene_id = extract_scene_id_from_path(video_path)\n",
    "\n",
    "        video = video.permute(1, 0, 2, 3)  # (T, C, H, W)\n",
    "        inputs = {\"pixel_values\": video.unsqueeze(0).to(device)}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = torch.nn.functional.softmax(model(**inputs).logits.squeeze(0), dim=-1).cpu()\n",
    "\n",
    "        scene_logits[scene_id].append(logits)\n",
    "        scene_labels[scene_id] = label\n",
    "\n",
    "    correct = 0\n",
    "    for scene_id, logits_list in scene_logits.items():\n",
    "        total_logits = torch.stack(logits_list).sum(dim=0)  # (num_classes,)\n",
    "        final_prediction = total_logits.argmax().item()\n",
    "        true_label = scene_labels[scene_id]\n",
    "\n",
    "        if final_prediction == true_label:\n",
    "            correct += 1\n",
    "\n",
    "    total = len(scene_logits)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Scene-level accuracy (softmax-sum voting): {accuracy:.4f}\")\n",
    "    return accuracy\n"
   ],
   "metadata": {
    "id": "pxWV5XqKmdi2"
   },
   "id": "pxWV5XqKmdi2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "from pathlib import Path\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def extract_scene_id_from_path(video_path: str) -> str:\n",
    "    filename = Path(video_path).stem\n",
    "    parts = filename.split('_')\n",
    "    return '_'.join(parts[:3])\n",
    "\n",
    "def evaluate_scene_level_model_topk_confidence(model, dataloader, video_paths, threshold=0.7, device: str = \"cuda\"):\n",
    "    \"\"\"\n",
    "    Scene-level evaluation using top-1 voting only for confident predictions (above threshold).\n",
    "\n",
    "    Args:\n",
    "        model: Hugging Face video classification model.\n",
    "        dataloader: PyTorch DataLoader.\n",
    "        video_paths: List of video file paths (ordered).\n",
    "        threshold: Min softmax confidence to count a chunk vote.\n",
    "        device: \"cuda\" or \"cpu\".\n",
    "\n",
    "    Returns:\n",
    "        float: Scene-level accuracy.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    scene_confident_preds = defaultdict(list)\n",
    "    scene_labels = {}\n",
    "\n",
    "    for idx, sample in enumerate(tqdm(dataloader)):\n",
    "        video = sample[\"video\"].squeeze(0)  # (C, T, H, W)\n",
    "        label = sample[\"label\"]\n",
    "        video_path = video_paths[idx]\n",
    "        scene_id = extract_scene_id_from_path(video_path)\n",
    "\n",
    "        video = video.permute(1, 0, 2, 3)  # (T, C, H, W)\n",
    "        inputs = {\"pixel_values\": video.unsqueeze(0).to(device)}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits.squeeze(0)  # (num_classes,)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            conf, pred = probs.max(dim=-1)\n",
    "\n",
    "        if conf.item() >= threshold:\n",
    "            scene_confident_preds[scene_id].append(pred.item())\n",
    "\n",
    "        scene_labels[scene_id] = label\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for scene_id, confident_preds in scene_confident_preds.items():\n",
    "        true = scene_labels[scene_id]\n",
    "        if confident_preds:\n",
    "            majority = Counter(confident_preds).most_common(1)[0][0]\n",
    "        else:\n",
    "            majority = true  # pessimistic fallback\n",
    "\n",
    "        if majority == true:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    print(f\"Scene-level accuracy (top-k voting, threshold={threshold}): {accuracy:.4f}\")\n",
    "    return accuracy\n"
   ],
   "metadata": {
    "id": "nZDNYbJ7oKsO"
   },
   "id": "nZDNYbJ7oKsO",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoProcessor, AutoModelForVideoClassification\n",
    "\n",
    "model_name = \"NiklasTUM/videomae-large-finetuned-deception-dataset_v2\"#\"NiklasTUM/videomae-base-finetuned-deception-dataset\"\n",
    "model = AutoModelForVideoClassification.from_pretrained(model_name).to(\"cuda\")\n",
    "model.eval()\n",
    "evaluate_direct_model(model=model, dataloader=test_dataset)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188,
     "referenced_widgets": [
      "0697462bb9034e06b1099d7ab69c99b1",
      "047acec9fbb24776a346bc0161c1d0ac",
      "dd6bd1ad6b724307a4fd33287558a6a5",
      "a51539872c1e4e21ab97425bdb84fa92",
      "8fdfa9599f0c4be4a8ca2986f473e2a7",
      "f80a630ce1824ee784d59f4285d5b77f",
      "30ea0a32eeff4e3ba91e87d44f18aedd",
      "39280fa2e8444816b69b6301fda1c44d",
      "b5365176ae7342d7aa4a29a0cd27ac40",
      "89366d47fffb4523afec615b215aef71",
      "ce1a0e03ece24b44a38ad0216443a787",
      "6b010677dcca410d87e8905fc6ec4448",
      "a2c7a532d8a54d36a53da5c080c700e2",
      "2377875a900a4a9a8d009455eb33cf62",
      "011863fb0dbe48f2a07627714c20f266",
      "9517faed549b4dba80d7c54199a86944",
      "9490bdf65e324efa9423792d8ea96e20",
      "14759238a32a4b308b8b819ae52621fd",
      "31e89a45ce434d9f9d7fb8542937cf80",
      "83528a48c8fd4aa9b3ce1ad1e9ef1504",
      "b02639c542fb489da0ea28690573041f",
      "ddde458d44ac49c78f6b8d9d66e58421"
     ]
    },
    "id": "HrOuuF9fQ6ip",
    "outputId": "ad803372-bb51-418f-87ac-b532a735b349"
   },
   "id": "HrOuuF9fQ6ip",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoProcessor, AutoModelForVideoClassification\n",
    "\n",
    "model_name = \"NiklasTUM/videomae-large-finetuned-deception-dataset_v2\"#\"NiklasTUM/videomae-base-finetuned-deception-dataset\"\n",
    "model = AutoModelForVideoClassification.from_pretrained(model_name).to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "video_paths = list(clip_to_label_test.keys())  # ordered!\n",
    "\n",
    "# Then later during eval:\n",
    "accuracy = evaluate_scene_level_model(\n",
    "    model=model,\n",
    "    dataloader=test_dataset,\n",
    "    video_paths=video_paths,\n",
    "    device=\"cuda\"\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M_tsUukgc9gS",
    "outputId": "862ec1e6-5b5d-4975-bc65-6ed07841134e"
   },
   "id": "M_tsUukgc9gS",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoProcessor, AutoModelForVideoClassification\n",
    "\n",
    "model_name = \"NiklasTUM/videomae-base-finetuned-deception-dataset\"\n",
    "model = AutoModelForVideoClassification.from_pretrained(model_name).to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "video_paths = list(clip_to_label_test.keys())  # ordered!\n",
    "\n",
    "# Then later during eval:\n",
    "accuracy = evaluate_scene_level_model_logit_sum(\n",
    "    model=model,\n",
    "    dataloader=test_dataset,\n",
    "    video_paths=video_paths,\n",
    "    device=\"cuda\"\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UvuHvYxXlInl",
    "outputId": "105941b3-ff36-47d9-d53a-00d2e5b117c5"
   },
   "id": "UvuHvYxXlInl",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoProcessor, AutoModelForVideoClassification\n",
    "\n",
    "model_name = \"NiklasTUM/videomae-base-finetuned-deception-dataset\"\n",
    "model = AutoModelForVideoClassification.from_pretrained(model_name).to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "video_paths = list(clip_to_label_test.keys())  # ordered!\n",
    "\n",
    "# Then later during eval:\n",
    "accuracy = evaluate_scene_level_model_softmax_sum(\n",
    "    model=model,\n",
    "    dataloader=test_dataset,\n",
    "    video_paths=video_paths,\n",
    "    device=\"cuda\"\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9dXP1T7mqpw",
    "outputId": "f501a101-cb88-4272-8814-544113255bb1"
   },
   "id": "b9dXP1T7mqpw",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoProcessor, AutoModelForVideoClassification\n",
    "\n",
    "model_name = \"NiklasTUM/videomae-base-finetuned-deception-dataset\"\n",
    "model = AutoModelForVideoClassification.from_pretrained(model_name).to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "video_paths = list(clip_to_label_test.keys())  # ordered!\n",
    "\n",
    "# Then later during eval:\n",
    "accuracy = evaluate_scene_level_model_topk_confidence(\n",
    "    model=model,\n",
    "    dataloader=test_dataset,\n",
    "    video_paths=video_paths,\n",
    "    threshold=0.6,\n",
    "    device=\"cuda\"\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "O9cnHU_DoRdh",
    "outputId": "874d1360-9387-49b4-9053-3dfa78fe645b"
   },
   "id": "O9cnHU_DoRdh",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "951054ac5a0c4301a7f56fc6e5566fb3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "VBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [],
      "layout": "IPY_MODEL_60229e31ccbc4d2086ac2302f3a0b985"
     }
    },
    "981c0d881ff74e5eb1558f07122569f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0aedd675651a4df7b3965af2a03cc93b",
      "placeholder": "​",
      "style": "IPY_MODEL_230154151a4f4eb88a926c39c117fe94",
      "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
     }
    },
    "52ee1b036150436ea59dd5c6cb9726b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "PasswordModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_736fbc68ab7746fc80f932d41b6e6903",
      "placeholder": "​",
      "style": "IPY_MODEL_0d880a9252974d76934efaba718e5608",
      "value": ""
     }
    },
    "b76377cb8cae4b9da8e4db54fc11925a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "CheckboxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "CheckboxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "CheckboxView",
      "description": "Add token as git credential?",
      "description_tooltip": null,
      "disabled": false,
      "indent": true,
      "layout": "IPY_MODEL_9e2a0927bbd14af29ba2f9860fbfb6a7",
      "style": "IPY_MODEL_bcdcba3f45c347a08e5a3de43f481919",
      "value": true
     }
    },
    "dfd3b11118ad4ae2ade6566d3e05de78": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ButtonModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_7b97da292510480caf55c8bf770c10bc",
      "style": "IPY_MODEL_dd086627509a413595f9c8b969458bc8",
      "tooltip": ""
     }
    },
    "2063d03e7309451497264575765dfdbd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_987d738767b74d5a977caa67281a9778",
      "placeholder": "​",
      "style": "IPY_MODEL_1b770f6a2d0f43efa68e84db257d34a6",
      "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
     }
    },
    "60229e31ccbc4d2086ac2302f3a0b985": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "0aedd675651a4df7b3965af2a03cc93b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "230154151a4f4eb88a926c39c117fe94": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "736fbc68ab7746fc80f932d41b6e6903": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0d880a9252974d76934efaba718e5608": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9e2a0927bbd14af29ba2f9860fbfb6a7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bcdcba3f45c347a08e5a3de43f481919": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7b97da292510480caf55c8bf770c10bc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd086627509a413595f9c8b969458bc8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ButtonStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "987d738767b74d5a977caa67281a9778": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b770f6a2d0f43efa68e84db257d34a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "88094895eba644cd8784c25ea5f797cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "LabelModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_75946fcf419b4c8abccf51e4f65a7a0f",
      "placeholder": "​",
      "style": "IPY_MODEL_01e62f01ce9941b489c1588e67698698",
      "value": "Connecting..."
     }
    },
    "75946fcf419b4c8abccf51e4f65a7a0f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "01e62f01ce9941b489c1588e67698698": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0697462bb9034e06b1099d7ab69c99b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_047acec9fbb24776a346bc0161c1d0ac",
       "IPY_MODEL_dd6bd1ad6b724307a4fd33287558a6a5",
       "IPY_MODEL_a51539872c1e4e21ab97425bdb84fa92"
      ],
      "layout": "IPY_MODEL_8fdfa9599f0c4be4a8ca2986f473e2a7"
     }
    },
    "047acec9fbb24776a346bc0161c1d0ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f80a630ce1824ee784d59f4285d5b77f",
      "placeholder": "​",
      "style": "IPY_MODEL_30ea0a32eeff4e3ba91e87d44f18aedd",
      "value": "config.json: 100%"
     }
    },
    "dd6bd1ad6b724307a4fd33287558a6a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_39280fa2e8444816b69b6301fda1c44d",
      "max": 928,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b5365176ae7342d7aa4a29a0cd27ac40",
      "value": 928
     }
    },
    "a51539872c1e4e21ab97425bdb84fa92": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_89366d47fffb4523afec615b215aef71",
      "placeholder": "​",
      "style": "IPY_MODEL_ce1a0e03ece24b44a38ad0216443a787",
      "value": " 928/928 [00:00&lt;00:00, 108kB/s]"
     }
    },
    "8fdfa9599f0c4be4a8ca2986f473e2a7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f80a630ce1824ee784d59f4285d5b77f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "30ea0a32eeff4e3ba91e87d44f18aedd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "39280fa2e8444816b69b6301fda1c44d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b5365176ae7342d7aa4a29a0cd27ac40": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "89366d47fffb4523afec615b215aef71": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce1a0e03ece24b44a38ad0216443a787": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6b010677dcca410d87e8905fc6ec4448": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a2c7a532d8a54d36a53da5c080c700e2",
       "IPY_MODEL_2377875a900a4a9a8d009455eb33cf62",
       "IPY_MODEL_011863fb0dbe48f2a07627714c20f266"
      ],
      "layout": "IPY_MODEL_9517faed549b4dba80d7c54199a86944"
     }
    },
    "a2c7a532d8a54d36a53da5c080c700e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9490bdf65e324efa9423792d8ea96e20",
      "placeholder": "​",
      "style": "IPY_MODEL_14759238a32a4b308b8b819ae52621fd",
      "value": "model.safetensors: 100%"
     }
    },
    "2377875a900a4a9a8d009455eb33cf62": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_31e89a45ce434d9f9d7fb8542937cf80",
      "max": 1215496248,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_83528a48c8fd4aa9b3ce1ad1e9ef1504",
      "value": 1215496248
     }
    },
    "011863fb0dbe48f2a07627714c20f266": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b02639c542fb489da0ea28690573041f",
      "placeholder": "​",
      "style": "IPY_MODEL_ddde458d44ac49c78f6b8d9d66e58421",
      "value": " 1.22G/1.22G [00:16&lt;00:00, 14.1MB/s]"
     }
    },
    "9517faed549b4dba80d7c54199a86944": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9490bdf65e324efa9423792d8ea96e20": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14759238a32a4b308b8b819ae52621fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "31e89a45ce434d9f9d7fb8542937cf80": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "83528a48c8fd4aa9b3ce1ad1e9ef1504": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b02639c542fb489da0ea28690573041f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ddde458d44ac49c78f6b8d9d66e58421": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
