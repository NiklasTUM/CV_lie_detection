{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KmK_goMP07c0",
    "outputId": "2cd959b3-edba-491b-b5e4-31df4ffb0428"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "#!pip install torch==2.1.0 torchvision==0.16.0 --upgrade --quiet"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K6aYjE8VRSS_",
    "outputId": "ea711266-c2cd-4e62-83c0-9d3858048b98"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LKanvO6E0en7",
    "outputId": "1ed6f278-a9be-429c-ac89-e89f1703fab0"
   },
   "source": [
    "!pip install transformers==4.40.0\n",
    "!pip install av\n",
    "!pip install imageio\n",
    "!pip install decord\n",
    "!pip install opencv-python\n",
    "!pip install flash-attn --no-build-isolation\n",
    "!pip install deepspeed"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install --upgrade deepspeed bitsandbytes\n",
    "\n",
    "!pip install -U huggingface_hub\n",
    "\n",
    "!pip install peft==0.10.0\n",
    "\n",
    "!pip install accelerate==0.28.0"
   ],
   "metadata": {
    "id": "rek8erbiR4_p"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "!pip uninstall -y numpy\n",
    "!pip install numpy==1.26.4 --no-cache-dir --force-reinstall\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9TDRfahEWnht",
    "outputId": "92c1cb71-3397-418e-a245-8fdc9a43f1b3"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "!python -m bitsandbytes"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_tPkDVL-7X_O",
    "outputId": "81a815b0-9a64-4d53-c203-e39c130d6cd2"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "!git clone https://github.com/OpenGVLab/InternVL.git"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Dzjaipd1-5w",
    "outputId": "ce5031e9-4e67-4ad9-ef84-04154bbd442a"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# !pip install -r /content/InternVL/requirements/internvl_chat.txt"
   ],
   "metadata": {
    "id": "yQg37vT56aGV"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from decord import VideoReader, cpu\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "# model setting\n",
    "model_path = \"OpenGVLab/InternVL2_5-2B\" # 'OpenGVLab/InternVideo2_5_Chat_8B'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_path, trust_remote_code=True).half().cuda().to(torch.bfloat16)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E5kqG2-N2W-I",
    "outputId": "75e5906a-d64f-4ca6-cf01-da9c0f0132f3"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def build_transform(input_size):\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([T.Lambda(lambda img: img.convert(\"RGB\") if img.mode != \"RGB\" else img), T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC), T.ToTensor(), T.Normalize(mean=MEAN, std=STD)])\n",
    "    return transform\n",
    "\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float(\"inf\")\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=6, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set((i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = ((i % (target_width // image_size)) * image_size, (i // (target_width // image_size)) * image_size, ((i % (target_width // image_size)) + 1) * image_size, ((i // (target_width // image_size)) + 1) * image_size)\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "\n",
    "def load_image(image, input_size=448, max_num=6):\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values\n",
    "\n",
    "\n",
    "def get_index(bound, fps, max_frame, first_idx=0, num_segments=32):\n",
    "    if bound:\n",
    "        start, end = bound[0], bound[1]\n",
    "    else:\n",
    "        start, end = -100000, 100000\n",
    "    start_idx = max(first_idx, round(start * fps))\n",
    "    end_idx = min(round(end * fps), max_frame)\n",
    "    seg_size = float(end_idx - start_idx) / num_segments\n",
    "    frame_indices = np.array([int(start_idx + (seg_size / 2) + np.round(seg_size * idx)) for idx in range(num_segments)])\n",
    "    return frame_indices\n",
    "\n",
    "def get_num_frames_by_duration(duration):\n",
    "        local_num_frames = 4\n",
    "        num_segments = int(duration // local_num_frames)\n",
    "        if num_segments == 0:\n",
    "            num_frames = local_num_frames\n",
    "        else:\n",
    "            num_frames = local_num_frames * num_segments\n",
    "\n",
    "        num_frames = min(512, num_frames)\n",
    "        num_frames = max(128, num_frames)\n",
    "\n",
    "        return num_frames\n",
    "\n",
    "def load_video(video_path, bound=None, input_size=448, max_num=1, num_segments=32, get_frame_by_duration = False):\n",
    "    vr = VideoReader(video_path, ctx=cpu(0), num_threads=1)\n",
    "    max_frame = len(vr) - 1\n",
    "    fps = float(vr.get_avg_fps())\n",
    "\n",
    "    pixel_values_list, num_patches_list = [], []\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    if get_frame_by_duration:\n",
    "        duration = max_frame / fps\n",
    "        num_segments = get_num_frames_by_duration(duration)\n",
    "    frame_indices = get_index(bound, fps, max_frame, first_idx=0, num_segments=num_segments)\n",
    "    for frame_index in frame_indices:\n",
    "        img = Image.fromarray(vr[frame_index].asnumpy()).convert(\"RGB\")\n",
    "        img = dynamic_preprocess(img, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "        pixel_values = [transform(tile) for tile in img]\n",
    "        pixel_values = torch.stack(pixel_values)\n",
    "        num_patches_list.append(pixel_values.shape[0])\n",
    "        pixel_values_list.append(pixel_values)\n",
    "    pixel_values = torch.cat(pixel_values_list)\n",
    "    return pixel_values, num_patches_list\n",
    "\n"
   ],
   "metadata": {
    "id": "Q6XRlPFI0nUc"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# evaluation setting\n",
    "max_num_frames = 512\n",
    "generation_config = dict(\n",
    "    do_sample=False,\n",
    "    temperature=0.0,\n",
    "    max_new_tokens=1024,\n",
    "    top_p=0.1,\n",
    "    num_beams=1\n",
    ")\n",
    "video_path = \"/content/drive/MyDrive/deception_detection/train/trial_lie_001_000.mp4\"\n",
    "num_segments=128\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "  torch.cuda.empty_cache()\n",
    "  import gc\n",
    "  gc.collect()\n",
    "  pixel_values, num_patches_list = load_video(video_path, num_segments=num_segments, max_num=1, get_frame_by_duration=False)\n",
    "  print(len(num_patches_list))\n",
    "  pixel_values = pixel_values.to(torch.bfloat16).to(model.device)\n",
    "  video_prefix = \"\".join([f\"Frame{i+1}: <image>\\n\" for i in range(len(num_patches_list))])\n",
    "  # single-turn conversation\n",
    "  question1 = \"Describe this video in detail.\"\n",
    "  question = video_prefix + question1\n",
    "  output1, chat_history = model.chat(tokenizer, pixel_values, question, generation_config, num_patches_list=num_patches_list, history=None, return_history=True)\n",
    "  print(output1)\n",
    "  del output1\n",
    "  torch.cuda.empty_cache()\n",
    "  import gc\n",
    "  gc.collect()\n",
    "  # # multi-turn conversation\n",
    "  question2 = \"How many people appear in the video?\"\n",
    "  output2, chat_history = model.chat(tokenizer, pixel_values, question, generation_config, num_patches_list=num_patches_list, history=chat_history, return_history=True)\n",
    "\n",
    "  print(output2)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IxfXc7yL2RC7",
    "outputId": "8b1e400b-a7af-4a7b-d9c3-f0b98ca4b40d"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "id": "CItVVmvtSTj3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Dict\n",
    "import random\n",
    "\n",
    "\n",
    "def build_internvl_video_jsonl(\n",
    "    video_dir: str,\n",
    "    output_jsonl_path: str,\n",
    "    prompt_text: str = \"Predict if the person in this video is lying (output \\\"lie\\\") or telling the truth (output \\\"truth\\\").\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Converts a folder of video clips into the InternVL chat data format (JSONL).\n",
    "\n",
    "    Args:\n",
    "        video_dir (str): Directory containing .mp4 video files (named with 'lie' or 'truth' in the name).\n",
    "        output_jsonl_path (str): File path to save the resulting JSONL dataset.\n",
    "        prompt_text (str): Text prompt for the human in each conversation.\n",
    "    \"\"\"\n",
    "    data_entries: List[Dict] = []\n",
    "    video_filenames = sorted(os.listdir(video_dir))\n",
    "    video_id = 0\n",
    "\n",
    "    for fname in video_filenames:\n",
    "        if not fname.endswith(\".mp4\"):\n",
    "            continue\n",
    "\n",
    "        parts = fname.split(\"_\")\n",
    "        if len(parts) < 3:\n",
    "            print(f\"Skipping malformed filename: {fname}\")\n",
    "            continue\n",
    "\n",
    "        label_str = parts[1].lower()\n",
    "        if label_str not in [\"lie\", \"truth\"]:\n",
    "            print(f\"Unknown label in filename: {fname}\")\n",
    "            continue\n",
    "\n",
    "        label = label_str  # 'lie' or 'truth'\n",
    "        video_path = fname  # relative path for InternVL\n",
    "\n",
    "        entry = {\n",
    "            \"id\": video_id,\n",
    "            \"video\": video_path,\n",
    "            \"conversations\": [\n",
    "                {\n",
    "                    \"from\": \"human\",\n",
    "                    \"value\": f\"<video>\\n{prompt_text}\"\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"gpt\",\n",
    "                    \"value\": label\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        data_entries.append(entry)\n",
    "        video_id += 1\n",
    "\n",
    "    random.shuffle(data_entries)\n",
    "    # Save to JSONL\n",
    "    with open(output_jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for entry in data_entries:\n",
    "            f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"Saved {len(data_entries)} entries to {output_jsonl_path}\")\n",
    "\n",
    "\n",
    "# Example usage (adjust paths in Colab!)\n",
    "ROOT_DIR = \"/content/drive/MyDrive/deception_detection/train\"\n",
    "OUTPUT_JSONL = \"/content/drive/MyDrive/deception_detection/internvl_train.jsonl\"\n",
    "\n",
    "build_internvl_video_jsonl(\n",
    "    video_dir=ROOT_DIR,\n",
    "    output_jsonl_path=OUTPUT_JSONL\n",
    ")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZRO7NK6L2Dmb",
    "outputId": "7708cb09-c16f-4279-a3ef-392b8857264d"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "meta = {\n",
    "    \"deception_train_set\": {\n",
    "        \"root\": \"/content/drive/MyDrive/deception_detection/train/\",\n",
    "        \"annotation\": \"/content/drive/MyDrive/deception_detection/internvl_train.jsonl\",\n",
    "        \"data_augment\": False,\n",
    "        \"max_dynamic_patch\": 4,\n",
    "        \"repeat_time\": 1,\n",
    "        \"length\": 500  # <- replace with actual count if known\n",
    "    }\n",
    "}\n",
    "os.makedirs(\"/content/drive/MyDrive/deception_detection\", exist_ok=True)\n",
    "\n",
    "meta_path = \"/content/drive/MyDrive/deception_detection/meta_train.json\"\n",
    "with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(\"✅ Meta file created:\", meta_path)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cZXqmfgQ2F1O",
    "outputId": "025daee1-71e8-464a-a633-dfb870cab63f"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "from huggingface_hub import HfApi, HfFolder, snapshot_download, create_repo, upload_folder\n",
    "\n",
    "# Parameters\n",
    "GPUS = 1\n",
    "BATCH_SIZE = 8\n",
    "PER_DEVICE_BATCH_SIZE = 2\n",
    "GRADIENT_ACC = BATCH_SIZE // PER_DEVICE_BATCH_SIZE // GPUS\n",
    "MASTER_PORT = 34229\n",
    "\n",
    "# Save to Google Drive\n",
    "HF_MODEL_NAME = \"InternVL2_5-2B-deception-finetuned\"  # \"internvideo2_5-8b-deception-finetuned\"\n",
    "DRIVE_OUTPUT_DIR = f\"/content/drive/MyDrive/deception_detection/{HF_MODEL_NAME}\"\n",
    "os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Also save locally for training compatibility\n",
    "LOCAL_OUTPUT_DIR = f\"work_dirs/internvl_chat_v2_5/{HF_MODEL_NAME}\"\n",
    "os.makedirs(LOCAL_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Export environment\n",
    "os.environ['PYTHONPATH'] = f\"{os.environ.get('PYTHONPATH', '')}:{os.getcwd()}\"\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "#os.environ['MASTER_PORT'] = str(MASTER_PORT)\n",
    "os.environ[\"PYTHONPATH\"] += \":/content/InternVL/internvl_chat\"\n",
    "os.environ[\"LAUNCHER\"] = \"pytorch\"\n",
    "\n",
    "!export LD_LIBRARY_PATH=/usr/lib64-nvidia:$LD_LIBRARY_PATH\n",
    "\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = \"/usr/lib64-nvidia:\" + os.environ.get(\"LD_LIBRARY_PATH\", \"\")\n"
   ],
   "metadata": {
    "id": "kBkix4CP2IKv"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "!nvidia-smi"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wMn2ZtQ968Dv",
    "outputId": "cdf050bc-7256-4625-c7cd-a9c367c8e767"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install decord==0.6.0 --no-cache-dir\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "riuooDuwdG51",
    "outputId": "7d00858c-5f67-4f0e-c427-e9a8e91a7b5c"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "CUSTOM_META_PATH = \"/content/drive/MyDrive/deception_detection/meta_train.json\"\n",
    "TRAIN_SCRIPT_PATH = \"InternVL/internvl_chat/internvl/train/internvl_chat_finetune.py\"\n",
    "DEEPSPEED_CONFIG = \"InternVL/internvl_chat/zero_stage1_config.json\"\n",
    "\n",
    "!CUDA_LAUNCH_BLOCKING=1 TORCH_DISTRIBUTED_DEBUG=DETAIL \\\n",
    "torchrun --nproc_per_node=1 \\\n",
    "  \"InternVL/internvl_chat/internvl/train/internvl_chat_finetune.py\" \\\n",
    "  --model_name_or_path \"OpenGVLab/InternVL2_5-2B\" \\\n",
    "  --conv_style \"internvl2_5\" \\\n",
    "  --use_fast_tokenizer False \\\n",
    "  --output_dir \"/content/drive/MyDrive/deception_detection/internvl2.5-2b-deception-finetuned\" \\\n",
    "  --meta_path \"/content/drive/MyDrive/deception_detection/meta_train.json\" \\\n",
    "  --overwrite_output_dir True \\\n",
    "  --force_image_size 224 \\\n",
    "  --max_dynamic_patch 4 \\\n",
    "  --down_sample_ratio 0.5 \\\n",
    "  --drop_path_rate 0.0 \\\n",
    "  --freeze_llm True \\\n",
    "  --freeze_mlp True \\\n",
    "  --freeze_backbone True \\\n",
    "  --use_llm_lora 16 \\\n",
    "  --vision_select_layer -1 \\\n",
    "  --dataloader_num_workers 2 \\\n",
    "  --fp16 True \\\n",
    "  --num_train_epochs 1 \\\n",
    "  --per_device_train_batch_size 1 \\\n",
    "  --gradient_accumulation_steps 4 \\\n",
    "  --evaluation_strategy \"no\" \\\n",
    "  --save_strategy \"steps\" \\\n",
    "  --save_steps 200 \\\n",
    "  --save_total_limit 1 \\\n",
    "  --learning_rate 4e-5 \\\n",
    "  --weight_decay 0.05 \\\n",
    "  --warmup_ratio 0.03 \\\n",
    "  --lr_scheduler_type \"cosine\" \\\n",
    "  --logging_steps 1 \\\n",
    "  --max_seq_length 8192 \\\n",
    "  --do_train True \\\n",
    "  --grad_checkpoint True \\\n",
    "  --group_by_length True \\\n",
    "  --dynamic_image_size True \\\n",
    "  --use_thumbnail True \\\n",
    "  --ps_version \"v2\" \\\n",
    "  --deepspeed \"InternVL/internvl_chat/zero_stage1_config.json\" \\\n",
    "  --report_to \"tensorboard\""
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "VYV2y0RR2Mu_",
    "outputId": "b68e26f8-564b-4225-e214-5f6cd4b00bed"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
